{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from catboost import CatBoostClassifier, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MFF Toolkit\n",
    "from ecg.data_processing import load_raw_ecg_data, load_and_process_ptb_xl_data, get_NORM_AF_data, oversample_data\n",
    "from ecg.training import train_model, extract_feature_vectors, ptb_xl_ecg_labeling\n",
    "from ecg.models import ECGModel, EGMModel\n",
    "from ablation.excel_processing import process_excel_files\n",
    "from ablation.data_loading import load_ablation_data, summarise_ablation_data\n",
    "from synthetic.synthetic_data import generate_synthetic_data, generate_synthetic_ecg_from_ptb_xl, generate_synthetic_egm, create_labels\n",
    "from integration.data_fusion import normalise_and_OHE_data, combine_data\n",
    "from evaluation.test_model_performance import cross_validate_model, compute_confidence_interval, plot_roc_and_pr_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ECG data import and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Define paths and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ptb_xl = \"\"\n",
    "input_folder_ablation_data = \"\"\n",
    "output_folder_ablation_data = \"\"\n",
    "sampling_rate_ecg=400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load ECG data from Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = load_and_process_ptb_xl_data(path_to_ptb_xl)\n",
    "Y_norm, Y_af = get_NORM_AF_data(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate NORM and AF data to create a pre-training dataset\n",
    "Y_ptb_xl = pd.concat([Y_norm, Y_af])\n",
    "\n",
    "# Random arrangement of the data\n",
    "Y_ptb_xl = Y_ptb_xl.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Load 12-lead ecg data of NORM and AF cases\n",
    "X_ptb_xl = load_raw_ecg_data(Y_ptb_xl, sampling_rate_ecg, path_to_ptb_xl)\n",
    "\n",
    "# Resampling the PTB-XL data for pre-training\n",
    "ptb_xl_X_res, ptb_xl_y_res = oversample_data(X_ptb_xl, Y_ptb_xl[\"Label\"])\n",
    "\n",
    "# Resampling results\n",
    "print(ptb_xl_X_res.shape)\n",
    "print(ptb_xl_y_res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create and pre-train ECG and EGM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load models and define DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ecg_model = ECGModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(ecg_model.parameters(), lr=0.001)\n",
    "\n",
    "ptb_xl_X_tensor = torch.tensor(ptb_xl_X_res, dtype=torch.float32)\n",
    "ptb_xl_y_tensor = torch.tensor(ptb_xl_ecg_labeling(ptb_xl_y_res), dtype=torch.long)\n",
    "ptb_xl_dataset = TensorDataset(ptb_xl_X_tensor, ptb_xl_y_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(ptb_xl_dataset))  # 80% train, 20% val\n",
    "val_size = len(ptb_xl_dataset) - train_size\n",
    "ptb_xl_train_dataset, ptb_xl_val_dataset = random_split(ptb_xl_dataset, [train_size, val_size])\n",
    "\n",
    "ptb_xl_train_dataloader = DataLoader(ptb_xl_train_dataset, batch_size=32, shuffle=True)\n",
    "ptb_xl_val_dataloader = DataLoader(ptb_xl_val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pre-train ECG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(ecg_model, device, ptb_xl_train_dataloader, ptb_xl_val_dataloader, criterion, optimizer, num_epochs=25, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Pre-train egm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load egm model here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create ablation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Preprocessing ablation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the given excel files. Replace all commas that are in decimal places with dots.\n",
    "process_excel_files(input_folder_ablation_data, output_folder_ablation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ablation data from the converted files\n",
    "temp_data, pressure_data, flow_data = load_ablation_data(output_folder_ablation_data, window=11, polyorder=2, delta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_data = summarise_ablation_data(temp_data)\n",
    "synthetic_data = generate_synthetic_data(sum_data, num_patients=499, random_state=42)\n",
    "ablation_data = pd.concat([sum_data, synthetic_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Generate synthetic surface ECG data by selecting some AF cases from PTB-XL (for testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synt_ptb_xl_ecgs = generate_synthetic_ecg_from_ptb_xl(Y_af, len(ablation_data))\n",
    "X_synt_ptb_xl_ecgs = load_raw_ecg_data(df_synt_ptb_xl_ecgs, sampling_rate_ecg, path_to_ptb_xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Generate synthetic intracardial ECG (EGM) data (for testing only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synt_egm_singals = generate_synthetic_egm(ablation_data, total_time=250, sampling_rate=1000, no_effect_ratio=0.5, min_effect=0.1, max_effect=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Create labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_labels = create_labels(synt_egm_singals, ablation_data, one_hot_encode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train ECG model and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_xl_synt_X_tensor = torch.tensor(X_synt_ptb_xl_ecgs, dtype=torch.float32)\n",
    "ptb_xl_synt_y_tensor = torch.tensor(ablation_labels, dtype=torch.long)\n",
    "ptb_xl_synt_dataset = TensorDataset(ptb_xl_synt_X_tensor, ptb_xl_synt_y_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(ptb_xl_synt_dataset))  # 80% train, 20% val\n",
    "val_size = len(ptb_xl_synt_dataset) - train_size\n",
    "ptb_xl_synt_train_dataset, ptb_xl_synt_val_dataset = random_split(ptb_xl_synt_dataset, [train_size, val_size])\n",
    "\n",
    "ptb_xl_synt_train_dataloader = DataLoader(ptb_xl_synt_train_dataset, batch_size=32, shuffle=True)\n",
    "ptb_xl_synt_val_dataloader = DataLoader(ptb_xl_synt_val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(ecg_model, device, ptb_xl_synt_train_dataloader, ptb_xl_synt_val_dataloader, criterion, optimizer, num_epochs=25, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature vectors ECG\n",
    "ptb_xl_synt_dataloader_vec = DataLoader(ptb_xl_synt_dataset, batch_size=32, shuffle=False)\n",
    "feat_vector_ecg = extract_feature_vectors(ecg_model, device, ptb_xl_synt_dataloader_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train EGM model and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape egm data for egm model\n",
    "ecg_signals_array = np.array([signal[1] for signal in synt_egm_singals])\n",
    "ecg_signals_array = ecg_signals_array.reshape(len(ablation_data), 250000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egm_model = EGMModel().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(egm_model.parameters(), lr=0.001)\n",
    "\n",
    "X_egm_tensor = torch.tensor(ecg_signals_array, dtype=torch.float32)\n",
    "y_egm_tensor = torch.tensor(ablation_labels, dtype=torch.long)\n",
    "y_egm_dataset = TensorDataset(X_egm_tensor, y_egm_tensor)\n",
    "\n",
    "train_size = int(0.8 * len(y_egm_dataset))  # 80% train, 20% val\n",
    "val_size = len(y_egm_dataset) - train_size\n",
    "egm_train_dataset, egm_val_dataset = random_split(y_egm_dataset, [train_size, val_size])\n",
    "\n",
    "emg_train_dataloader = DataLoader(egm_train_dataset, batch_size=32, shuffle=True)\n",
    "emg_val_dataloader = DataLoader(egm_val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(egm_model, device, emg_train_dataloader, emg_val_dataloader, criterion, optimizer, num_epochs=25, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature vector EGM\n",
    "synt_egm_dataloader_vec = DataLoader(y_egm_dataset, batch_size=32, shuffle=False)\n",
    "feat_vector_egm = extract_feature_vectors(egm_model, device, synt_egm_dataloader_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Fusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_vars = [\n",
    "    'AblationCount', 'NadirTemperature_mean', 'NadirTemperature_median', 'NadirTemperature_var',\n",
    "    'AblationTime_mean', 'AblationTime_median', 'AblationTime_var',\n",
    "    't_end_mean', 't_end_median', 't_end_var'\n",
    "]\n",
    "\n",
    "categorical_vars = ['Gender', 'RSPV', 'LIPV', 'LSPV', 'RIPV', 'Diagnosis']\n",
    "\n",
    "norm_ablation_data = normalise_and_OHE_data(ablation_data, continuous_vars, categorical_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Combine all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = combine_data(pd.DataFrame(feat_vector_ecg), pd.DataFrame(feat_vector_egm), None, norm_ablation_data.drop(columns=[\"PatientId\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Train CatBoost model and perform cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, preds, pred_probs, roc_aucs, all_confusion_matrices, cm_scores = cross_validate_model(np.array(final_df), np.array(np.argmax(ablation_labels, axis=1)), n_splits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fi_train, X_fi_test, y_fi_train, y_fi_test = train_test_split(np.array(final_df), np.array(np.argmax(ablation_labels, axis=1)), test_size=0.2, random_state=42)\n",
    "\n",
    "train_pool = Pool(data=X_fi_train, label=y_fi_train)\n",
    "test_pool = Pool(data=X_fi_test, label=y_fi_test)\n",
    "\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=2, verbose=10)\n",
    "catboost_model.fit(train_pool)\n",
    "\n",
    "y_pred = catboost_model.predict(test_pool)\n",
    "y_pred_prob = catboost_model.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_fi_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_fi_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Feature Importances\n",
    "feature_importances = catboost_model.get_feature_importance(train_pool)\n",
    "\n",
    "# Ensure feature names\n",
    "feature_names = np.array(final_df.columns, dtype=str)\n",
    "\n",
    "# Define indices for the groups\n",
    "group_1_indices = np.arange(0, 128)\n",
    "group_2_indices = np.arange(128, 256)\n",
    "other_indices = np.arange(256, len(feature_names))\n",
    "\n",
    "# Calculate aggregated feature importances\n",
    "group_1_importance = np.sum(feature_importances[group_1_indices])\n",
    "group_2_importance = np.sum(feature_importances[group_2_indices])\n",
    "\n",
    "# Directly adopt feature importances for the remaining features\n",
    "other_importances = feature_importances[other_indices]\n",
    "\n",
    "# Labels for the diagram\n",
    "final_feature_names = [\"ECG\", \"EGM\"] + list(feature_names[other_indices])\n",
    "final_importances = [group_1_importance, group_2_importance] + list(other_importances)\n",
    "\n",
    "# Plot diagram\n",
    "plt.figure(figsize=(5, 6))\n",
    "plt.barh(final_feature_names, final_importances, align='center')\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
